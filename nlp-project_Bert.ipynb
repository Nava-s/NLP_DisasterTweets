{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":17777,"databundleVersionId":869809},{"sourceType":"modelInstanceVersion","sourceId":6054,"databundleVersionId":7429172,"modelInstanceId":4675},{"sourceType":"modelInstanceVersion","sourceId":6068,"databundleVersionId":7429247,"modelInstanceId":4689}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install wurlitzer\n#!pip install keras-core --upgrade\n#!pip install -q keras-nlp --upgrade\n#!pip install tensorflow --upgrade\n#!pip install tensorflow[and-cuda]\n#!pip install tensorflow-text\n# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:22.880261Z","iopub.execute_input":"2024-05-09T06:25:22.881090Z","iopub.status.idle":"2024-05-09T06:25:22.886299Z","shell.execute_reply.started":"2024-05-09T06:25:22.881056Z","shell.execute_reply":"2024-05-09T06:25:22.885323Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom transformers import BertTokenizer, BertModel\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras import losses, metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras_nlp\nimport keras\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T06:25:22.895117Z","iopub.execute_input":"2024-05-09T06:25:22.895404Z","iopub.status.idle":"2024-05-09T06:25:30.462714Z","shell.execute_reply.started":"2024-05-09T06:25:22.895380Z","shell.execute_reply":"2024-05-09T06:25:30.461675Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-09 06:25:27.199036: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-09 06:25:27.199095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-09 06:25:27.200598: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\nTensorFlow version: 2.15.0\nKerasNLP version: 0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(train_df.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train_df.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(test_df.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test_df.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.464970Z","iopub.execute_input":"2024-05-09T06:25:30.465782Z","iopub.status.idle":"2024-05-09T06:25:30.509024Z","shell.execute_reply.started":"2024-05-09T06:25:30.465751Z","shell.execute_reply":"2024-05-09T06:25:30.508105Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training Set Shape = (7613, 5)\nTraining Set Memory Usage = 0.29 MB\nTest Set Shape = (3263, 4)\nTest Set Memory Usage = 0.10 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_df.isnull().sum())\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.510380Z","iopub.execute_input":"2024-05-09T06:25:30.510693Z","iopub.status.idle":"2024-05-09T06:25:30.527743Z","shell.execute_reply.started":"2024-05-09T06:25:30.510668Z","shell.execute_reply":"2024-05-09T06:25:30.526756Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(test_df.isnull().sum())\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.529012Z","iopub.execute_input":"2024-05-09T06:25:30.529634Z","iopub.status.idle":"2024-05-09T06:25:30.541112Z","shell.execute_reply.started":"2024-05-09T06:25:30.529601Z","shell.execute_reply":"2024-05-09T06:25:30.540312Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"id             0\nkeyword       26\nlocation    1105\ntext           0\ndtype: int64\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"BATCH_SIZE = 16\nNUM_TRAINING_EXAMPLES = train_df.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.544522Z","iopub.execute_input":"2024-05-09T06:25:30.545032Z","iopub.status.idle":"2024-05-09T06:25:30.550688Z","shell.execute_reply.started":"2024-05-09T06:25:30.544999Z","shell.execute_reply":"2024-05-09T06:25:30.549658Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_df[\"text\"]\ny = train_df[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = test_df[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.551851Z","iopub.execute_input":"2024-05-09T06:25:30.552131Z","iopub.status.idle":"2024-05-09T06:25:30.563829Z","shell.execute_reply.started":"2024-05-09T06:25:30.552107Z","shell.execute_reply":"2024-05-09T06:25:30.562993Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:30.565243Z","iopub.execute_input":"2024-05-09T06:25:30.565833Z","iopub.status.idle":"2024-05-09T06:25:44.988659Z","shell.execute_reply.started":"2024-05-09T06:25:30.565801Z","shell.execute_reply":"2024-05-09T06:25:44.987861Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Attaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'preprocessor.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'assets/tokenizer/vocabulary.txt' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'task.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'metadata.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'config.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\nAttaching 'model.weights.h5' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/backbone.py:101: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  return id(getattr(self, attr)) not in self._functional_layer_ids\n/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/backbone.py:101: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  return id(getattr(self, attr)) not in self._functional_layer_ids\n","output_type":"stream"}]},{"cell_type":"code","source":"# Compile\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n    optimizer=keras.optimizers.Adam(1e-4),\n    metrics= [\"accuracy\"]  \n)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:44.990006Z","iopub.execute_input":"2024-05-09T06:25:44.990686Z","iopub.status.idle":"2024-05-09T06:25:45.006652Z","shell.execute_reply.started":"2024-05-09T06:25:44.990649Z","shell.execute_reply":"2024-05-09T06:25:45.005881Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Fit\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=16,\n                         epochs=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:25:45.007719Z","iopub.execute_input":"2024-05-09T06:25:45.008003Z","iopub.status.idle":"2024-05-09T06:27:50.605677Z","shell.execute_reply.started":"2024-05-09T06:25:45.007969Z","shell.execute_reply":"2024-05-09T06:27:50.603969Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715235975.926402     225 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"215/381 [===============>..............] - ETA: 1:12 - loss: 0.4626 - accuracy: 0.8020","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                         \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py:194\u001b[0m, in \u001b[0;36mPipelineModel.fit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m         (vx, vy, vsw) \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(\n\u001b[1;32m    188\u001b[0m             validation_data\n\u001b[1;32m    189\u001b[0m         )\n\u001b[1;32m    190\u001b[0m         validation_data \u001b[38;5;241m=\u001b[39m _convert_inputs_to_dataset(\n\u001b[1;32m    191\u001b[0m             vx, vy, vsw, batch_size\n\u001b[1;32m    192\u001b[0m         )\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_182/245841519.py\", line 2, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py\", line 194, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\nFailed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.graph.launch' failed: CaptureGpuGraph failed (the requested functionality is not supported; current tracing scope: custom-call.147): INTERNAL: Failed to end stream capture: CUDA_ERROR_STREAM_CAPTURE_INVALIDATED: operation failed due to a previous error during capture; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_17187__.14337,program_id=7395#.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_17830]"],"ename":"InternalError","evalue":"Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/tmp/ipykernel_182/245841519.py\", line 2, in <module>\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras_nlp/src/utils/pipeline_model.py\", line 194, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\nFailed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.graph.launch' failed: CaptureGpuGraph failed (the requested functionality is not supported; current tracing scope: custom-call.147): INTERNAL: Failed to end stream capture: CUDA_ERROR_STREAM_CAPTURE_INVALIDATED: operation failed due to a previous error during capture; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_17187__.14337,program_id=7395#.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_train_function_17830]","output_type":"error"}]},{"cell_type":"code","source":"#loss, binary_accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n#print('Train','\\nLoss value: '+ str(round(loss,2)),'\\nAccuracy: ' + str(round(binary_accuracy,2)))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.606453Z","iopub.status.idle":"2024-05-09T06:27:50.606804Z","shell.execute_reply.started":"2024-05-09T06:27:50.606635Z","shell.execute_reply":"2024-05-09T06:27:50.606650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions_train_nn = classifier.predict(train_df['text'])\npredictions_test_nn = classifier.predict(test_df['text'])\n#predictions_val_nn = model.predict(x_val)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.608942Z","iopub.status.idle":"2024-05-09T06:27:50.609333Z","shell.execute_reply.started":"2024-05-09T06:27:50.609125Z","shell.execute_reply":"2024-05-09T06:27:50.609139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        y_pred,  # Rimuovi np.argmax\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.610551Z","iopub.status.idle":"2024-05-09T06:27:50.610887Z","shell.execute_reply.started":"2024-05-09T06:27:50.610722Z","shell.execute_reply":"2024-05-09T06:27:50.610736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_train_nn = np.array(list(map(lambda x: 0 if x <= 0.5 else 1, predictions_train_nn.flatten())))\n#predictions_val_nn = np.array(list(map(lambda x: 0 if x <= 0.5 else 1, predictions_val_nn.flatten())))\npredictions_test_nn = np.array(list(map(lambda x: 0 if x <= 0.5 else 1, predictions_test_nn.flatten())))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.612845Z","iopub.status.idle":"2024-05-09T06:27:50.613694Z","shell.execute_reply.started":"2024-05-09T06:27:50.613432Z","shell.execute_reply":"2024-05-09T06:27:50.613453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_train_nn = predictions_train_nn.reshape(-1,1)\n#predictions_val_nn = predictions_val_nn.reshape(-1,1)\npredictions_test_nn = predictions_test_nn.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.614887Z","iopub.status.idle":"2024-05-09T06:27:50.615347Z","shell.execute_reply.started":"2024-05-09T06:27:50.615096Z","shell.execute_reply":"2024-05-09T06:27:50.615115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"displayConfusionMatrix(train_df['target'], predictions_train_nn, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.616496Z","iopub.status.idle":"2024-05-09T06:27:50.616946Z","shell.execute_reply.started":"2024-05-09T06:27:50.616714Z","shell.execute_reply":"2024-05-09T06:27:50.616733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = predictions_test_nn.reshape(-1,)\noutput.shape\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.618604Z","iopub.status.idle":"2024-05-09T06:27:50.619049Z","shell.execute_reply.started":"2024-05-09T06:27:50.618821Z","shell.execute_reply":"2024-05-09T06:27:50.618840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'Id': test_df['id'], 'target': output})","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.620584Z","iopub.status.idle":"2024-05-09T06:27:50.621024Z","shell.execute_reply.started":"2024-05-09T06:27:50.620804Z","shell.execute_reply":"2024-05-09T06:27:50.620823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('sample_submission.csv', index=False, sep=',')","metadata":{"execution":{"iopub.status.busy":"2024-05-09T06:27:50.622732Z","iopub.status.idle":"2024-05-09T06:27:50.623217Z","shell.execute_reply.started":"2024-05-09T06:27:50.622948Z","shell.execute_reply":"2024-05-09T06:27:50.622966Z"},"trusted":true},"execution_count":null,"outputs":[]}]}